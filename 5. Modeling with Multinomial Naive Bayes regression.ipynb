{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with Multinomial Naive Bayes regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.compose import  ColumnTransformer, make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X\n",
    "%store -r y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3020, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemmatokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformer_cv = make_column_transformer(\n",
    "  (CountVectorizer(tokenizer = LemmaTokenizer()), 'cleaned'),\n",
    "  remainder = 'passthrough'\n",
    ")\n",
    "col_transformer_td = make_column_transformer(\n",
    "  (TfidfVectorizer(tokenizer = LemmaTokenizer()), 'cleaned'),\n",
    "  remainder = 'passthrough'\n",
    ")\n",
    "pipe_mnb_cv = Pipeline([\n",
    "    (\"col_trans\", col_transformer_cv),\n",
    "    ('mnb', MultinomialNB())])\n",
    "\n",
    "pipe_mnb_td = Pipeline([\n",
    "    (\"col_trans\", col_transformer_td),\n",
    "    ('mnb', MultinomialNB())])\n",
    "    \n",
    "# Construct Grid Parameters for CountVectorizer\n",
    "hyperparams_cv = {\n",
    "               'col_trans__countvectorizer__ngram_range': [(1,1), (1,2)],\n",
    "               'col_trans__countvectorizer__stop_words': [None,'english'],\n",
    "               'col_trans__countvectorizer__max_features': [None, 100, 500,1000],\n",
    "               'col_trans__countvectorizer__min_df': [1, 3, 4],\n",
    "               'col_trans__countvectorizer__max_df': [0.9, 0.95, .9],\n",
    "\n",
    "                 }\n",
    "# Construct Grid Parameters for TDIDFFVectorizer\n",
    "hyperparams_td = {'col_trans__tfidfvectorizer__ngram_range': [(1,1), (1,2)],\n",
    "               'col_trans__tfidfvectorizer__stop_words': [None, 'english'],\n",
    "               'col_trans__tfidfvectorizer__max_features': [None, 100, 500,1000],\n",
    "               'col_trans__tfidfvectorizer__min_df': [1, 3, 4],\n",
    "               'col_trans__tfidfvectorizer__max_df': [0.9, 0.95, .99]\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes & countvectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   50.5s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done 720 out of 720 | elapsed:  3.1min finished\n",
      "/Users/kalz/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('col_trans',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='passthrough',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('countvectorizer',\n",
       "                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                         binary=False,\n",
       "                                                                                         decode_error='strict',\n",
       "                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                         encoding='utf-8',\n",
       "                                                                                         input='content',\n",
       "                                                                                         lower...\n",
       "             param_grid={'col_trans__countvectorizer__max_df': [0.9, 0.95, 0.9],\n",
       "                         'col_trans__countvectorizer__max_features': [None, 100,\n",
       "                                                                      500,\n",
       "                                                                      1000],\n",
       "                         'col_trans__countvectorizer__min_df': [1, 3, 4],\n",
       "                         'col_trans__countvectorizer__ngram_range': [(1, 1),\n",
       "                                                                     (1, 2)],\n",
       "                         'col_trans__countvectorizer__stop_words': [None,\n",
       "                                                                    'english']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb_cv = GridSearchCV(pipe_mnb_cv, param_grid= hyperparams_cv, verbose=1,cv=5,n_jobs=4)\n",
    "gs_mnb_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9130242825607064\n",
      "Test score: 0.8649006622516556\n"
     ]
    }
   ],
   "source": [
    "print(f'Train score: {gs_mnb_cv.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs_mnb_cv.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'col_trans__countvectorizer__max_df': 0.9,\n",
       " 'col_trans__countvectorizer__max_features': 1000,\n",
       " 'col_trans__countvectorizer__min_df': 3,\n",
       " 'col_trans__countvectorizer__ngram_range': (1, 1),\n",
       " 'col_trans__countvectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters\n",
    "gs_mnb_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_bitcoin</th>\n",
       "      <th>predict_ethereum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_bitcoin</th>\n",
       "      <td>256</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_ethereum</th>\n",
       "      <td>74</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 predict_bitcoin  predict_ethereum\n",
       "actual_bitcoin               256                28\n",
       "actual_ethereum               74               397"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion Matrix\n",
    "pred_y_cv = gs_mnb_cv.predict(X_test)\n",
    "pd.DataFrame(confusion_matrix(y_test, pred_y_cv), \n",
    "             columns=['predict_bitcoin', 'predict_ethereum'], \n",
    "             index=['actual_bitcoin', 'actual_ethereum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes & TFIDFVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 720 out of 720 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('col_trans',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='passthrough',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('tfidfvectorizer',\n",
       "                                                                         TfidfVectorizer(analyzer='word',\n",
       "                                                                                         binary=False,\n",
       "                                                                                         decode_error='strict',\n",
       "                                                                                         dtype=<class 'numpy.float64'>,\n",
       "                                                                                         encoding='utf-8',\n",
       "                                                                                         input='content',\n",
       "                                                                                         low...\n",
       "             param_grid={'col_trans__tfidfvectorizer__max_df': [0.9, 0.95,\n",
       "                                                                0.99],\n",
       "                         'col_trans__tfidfvectorizer__max_features': [None, 100,\n",
       "                                                                      500,\n",
       "                                                                      1000],\n",
       "                         'col_trans__tfidfvectorizer__min_df': [1, 3, 4],\n",
       "                         'col_trans__tfidfvectorizer__ngram_range': [(1, 1),\n",
       "                                                                     (1, 2)],\n",
       "                         'col_trans__tfidfvectorizer__stop_words': [None,\n",
       "                                                                    'english']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb_td = GridSearchCV(pipe_mnb_td, param_grid= hyperparams_td, verbose=1,cv=5,n_jobs=4)\n",
    "gs_mnb_td.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8984547461368654\n",
      "Test score: 0.856953642384106\n"
     ]
    }
   ],
   "source": [
    "print(f'Train score: {gs_mnb_td.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs_mnb_td.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'col_trans__tfidfvectorizer__max_df': 0.9,\n",
       " 'col_trans__tfidfvectorizer__max_features': 1000,\n",
       " 'col_trans__tfidfvectorizer__min_df': 1,\n",
       " 'col_trans__tfidfvectorizer__ngram_range': (1, 2),\n",
       " 'col_trans__tfidfvectorizer__stop_words': None}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters\n",
    "gs_mnb_td.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-a3d8c5e62fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgs_mnb_td\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "gs_mnb_td.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-b920d795718c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#find the best estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs_mnb_td\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mnb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#find the best estimators\n",
    "pd.DataFrame(gs_mnb_td.best_estimator_.named_steps['mnb'].coef_, columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_bitcoin</th>\n",
       "      <th>predict_ethereum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_bitcoin</th>\n",
       "      <td>204</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_ethereum</th>\n",
       "      <td>28</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 predict_bitcoin  predict_ethereum\n",
       "actual_bitcoin               204                80\n",
       "actual_ethereum               28               443"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion Matrix\n",
    "pred_y_td = gs_mnb_td.predict(X_test)\n",
    "pd.DataFrame(confusion_matrix(y_test, pred_y_td), \n",
    "             columns=['predict_bitcoin', 'predict_ethereum'], \n",
    "             index=['actual_bitcoin', 'actual_ethereum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Multinomial Naive Bayes regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  \t|TFIDFVectorizer  \t|CountVectorizer  \t|\n",
    "|---\t|---\t|---\t|\n",
    "|max_df  \t|0.9  \t|0.9  \t|\n",
    "|min_df  \t|1  \t|3  \t|\n",
    "|max_features  \t|1000  \t|1000  \t|\n",
    "|ngram_range  \t|(1, 2)  \t|(1, 1)  \t|\n",
    "|stop_words  \t|None  \t|english  \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  \t|TFIDFVectorizer  \t|CountVectorizer  \t|\n",
    "|---\t|---\t|---\t|\n",
    "|Train   \t|0.898  \t|0.913  \t|\n",
    "|Test  \t|0.857  \t|0.865  \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TFIDFVectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  \t|predict_bitcoin  \t|predict_ethereum  \t|\n",
    "|---\t|---\t|---\t|\n",
    "|actual_bitcoin  \t|215  \t|69  \t|\n",
    "|actual_ethereum\t  \t|43  \t|428  \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CountVectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  \t|predict_bitcoin  \t|predict_ethereum  \t|\n",
    "|---\t|---\t|---\t|\n",
    "|actual_bitcoin  \t|256  \t|28  \t|\n",
    "|actual_ethereum\t  \t|74  \t|397  \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision = TP / (TP + FP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|MODEL|VECTORIZER  \t|TP  \t|TN  \t|FP  \t|FN  \t|Precision  \t|\n",
    "|---|---\t|---\t|---\t|---\t|---\t|---\t|\n",
    "|MNB|TFIDF  \t|204  \t|443  \t|28  \t|80  \t|0.879  \t|\n",
    "||COUNT  \t|256  \t|397  \t|74  \t|28  \t|0.776  \t|\n",
    "|LOG REG|TFIDF  \t|220  \t|436  \t|35  \t|64  \t|0.863  \t|\n",
    "||COUNT  \t|254  \t|419  \t|52  \t|30  \t|0.830  \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_train' (DataFrame)\n",
      "Stored 'y' (Series)\n"
     ]
    }
   ],
   "source": [
    "%store X_train\n",
    "%store y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'y_train' (Series)\n"
     ]
    }
   ],
   "source": [
    "%store y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
